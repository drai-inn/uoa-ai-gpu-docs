{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Index","text":"Research AI GPU Platform Accelerating research with high-performance computing, scalable infrastructure, and secure management. Get Started View Architecture <ul> <li> <p> Scalable Infrastructure</p> <p>Built on Kubernetes to provide flexible and scalable GPU resources for diverse research workloads, from training to inference.</p> </li> <li> <p> Secure &amp; Managed</p> <p>Integrated with Keycloak for secure authentication and robust policy management, ensuring your research data remains protected.</p> </li> <li> <p> Research Focused</p> <p>Tailored specifically to support the needs of academic research, providing the compute power required for breakthrough discoveries.</p> </li> <li> <p> Community Driven</p> <p>Built by researchers, for researchers. Contribute your own guides, fix issues, and help shape the platform.</p> </li> </ul>","path":["Index"],"tags":[]},{"location":"#quick-links","level":2,"title":"Quick Links","text":"<ul> <li> <p>Running LLMs on NeSI     ---     Step-by-step guide to deploying Large Language Models.</p> </li> <li> <p>Platform Overview     ---     Deep dive into the cluster architecture and components.</p> </li> <li> <p>Benchmarks     ---     Performance metrics and comparison data.</p> </li> <li> <p>Community &amp; Contributing     ---     Join the community, report issues, and contribute guides.</p> </li> </ul>","path":["Index"],"tags":[]},{"location":"guides/llm-on-nesi/","level":1,"title":"Running LLMs on NeSI","text":"<p>This guide provides an overview of how to deploy and run Large Language Models (LLMs) on the NeSI HPC platform using the UoA AI GPU resources.</p>","path":["Guides","Running LLMs on NeSI"],"tags":[]},{"location":"guides/llm-on-nesi/#prerequisites","level":2,"title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Docker Engine: Installed locally (e.g., Docker Desktop).</li> <li>SSH Access: Configured SSH keys for accessing the VM (<code>GatewayPorts yes</code> required in <code>sshd_config</code>).</li> <li>DuckDNS: A domain and API key for external access.</li> <li>API Keys: OpenAI, LiteLLM, etc., as needed for your specific model.</li> </ul>","path":["Guides","Running LLMs on NeSI"],"tags":[]},{"location":"guides/llm-on-nesi/#quick-start","level":2,"title":"Quick Start","text":"<ol> <li>Configuration: Create a <code>.env</code> file with your secrets (API keys, passwords).</li> <li>Launch: Run <code>sudo docker compose up -d</code> to start the services.</li> <li>HPC Job Submission: Use the provided Slurm scripts to submit your model inference job to the HPC nodes.</li> </ol>","path":["Guides","Running LLMs on NeSI"],"tags":[]},{"location":"guides/llm-on-nesi/#repository","level":2,"title":"Repository","text":"<p>For the full tutorial, example <code>.env</code> files, and Slurm submission scripts, please refer to the source repository:</p> <p>drai-inn/llm-nesi-example</p>","path":["Guides","Running LLMs on NeSI"],"tags":[]},{"location":"platform/overview/","level":1,"title":"Platform Architecture","text":"<p>The UoA Research AI GPU Platform is designed to provide a scalable and secure environment for AI research.</p>","path":["Platform","Platform Architecture"],"tags":[]},{"location":"platform/overview/#overview","level":2,"title":"Overview","text":"<p>The platform is built as a Kubernetes (K8s) environment, integrating various components to manage user access, security, and resource allocation.</p>","path":["Platform","Platform Architecture"],"tags":[]},{"location":"platform/overview/#key-components","level":3,"title":"Key Components","text":"<ul> <li>Kubernetes Cluster: The core orchestration layer for managing containerized AI workloads.</li> <li>Keycloak: Handles user authentication and role-based access control (RBAC).</li> <li>Policies: Defines usage policies and resource limits for different user roles.</li> </ul>","path":["Platform","Platform Architecture"],"tags":[]},{"location":"platform/overview/#repository","level":2,"title":"Repository","text":"<p>For detailed configuration files, including K8s manifests, Keycloak setup, and policy definitions, please refer to the source repository:</p> <p>drai-inn/ai-gpu-platform</p>","path":["Platform","Platform Architecture"],"tags":[]},{"location":"reference/benchmarks/","level":1,"title":"AI Benchmarks","text":"<p>This section tracks performance benchmarks for various AI models and hardware configurations on the platform.</p>","path":["Reference","AI Benchmarks"],"tags":[]},{"location":"reference/benchmarks/#repository","level":2,"title":"Repository","text":"<p>We maintain a collection of benchmark scripts and results in the following repository:</p> <p>drai-inn/ai-benchmarks</p> <p>Note: Detailed benchmark results and methodologies will be populated here as they become available.</p>","path":["Reference","AI Benchmarks"],"tags":[]}]}