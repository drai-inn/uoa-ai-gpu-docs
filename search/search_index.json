{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Research AI GPU Platform Accelerating research with high-performance computing, scalable infrastructure, and secure management. Get Started View Architecture <ul> <li> <p> Scalable Infrastructure</p> <p>Built on Kubernetes to provide flexible and scalable GPU resources for diverse research workloads, from training to inference.</p> </li> <li> <p> Secure &amp; Managed</p> <p>Integrated with Keycloak for secure authentication and robust policy management, ensuring your research data remains protected.</p> </li> <li> <p> Research Focused</p> <p>Tailored specifically to support the needs of academic research, providing the compute power required for breakthrough discoveries.</p> </li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>Running LLMs on NeSI     ---     Step-by-step guide to deploying Large Language Models.</p> </li> <li> <p>Platform Overview     ---     Deep dive into the cluster architecture and components.</p> </li> <li> <p>Benchmarks     ---     Performance metrics and comparison data.</p> </li> </ul>"},{"location":"guides/llm-on-nesi/","title":"Running LLMs on NeSI","text":"<p>This guide provides an overview of how to deploy and run Large Language Models (LLMs) on the NeSI HPC platform using the UoA AI GPU resources.</p>"},{"location":"guides/llm-on-nesi/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Docker Engine: Installed locally (e.g., Docker Desktop).</li> <li>SSH Access: Configured SSH keys for accessing the VM (<code>GatewayPorts yes</code> required in <code>sshd_config</code>).</li> <li>DuckDNS: A domain and API key for external access.</li> <li>API Keys: OpenAI, LiteLLM, etc., as needed for your specific model.</li> </ul>"},{"location":"guides/llm-on-nesi/#quick-start","title":"Quick Start","text":"<ol> <li>Configuration: Create a <code>.env</code> file with your secrets (API keys, passwords).</li> <li>Launch: Run <code>sudo docker compose up -d</code> to start the services.</li> <li>HPC Job Submission: Use the provided Slurm scripts to submit your model inference job to the HPC nodes.</li> </ol>"},{"location":"guides/llm-on-nesi/#repository","title":"Repository","text":"<p>For the full tutorial, example <code>.env</code> files, and Slurm submission scripts, please refer to the source repository:</p> <p>drai-inn/llm-nesi-example</p>"},{"location":"platform/overview/","title":"Platform Architecture","text":"<p>The UoA Research AI GPU Platform is designed to provide a scalable and secure environment for AI research.</p>"},{"location":"platform/overview/#overview","title":"Overview","text":"<p>The platform is built as a Kubernetes (K8s) environment, integrating various components to manage user access, security, and resource allocation.</p>"},{"location":"platform/overview/#key-components","title":"Key Components","text":"<ul> <li>Kubernetes Cluster: The core orchestration layer for managing containerized AI workloads.</li> <li>Keycloak: Handles user authentication and role-based access control (RBAC).</li> <li>Policies: Defines usage policies and resource limits for different user roles.</li> </ul>"},{"location":"platform/overview/#repository","title":"Repository","text":"<p>For detailed configuration files, including K8s manifests, Keycloak setup, and policy definitions, please refer to the source repository:</p> <p>drai-inn/ai-gpu-platform</p>"},{"location":"reference/benchmarks/","title":"AI Benchmarks","text":"<p>This section tracks performance benchmarks for various AI models and hardware configurations on the platform.</p>"},{"location":"reference/benchmarks/#repository","title":"Repository","text":"<p>We maintain a collection of benchmark scripts and results in the following repository:</p> <p>drai-inn/ai-benchmarks</p> <p>Note: Detailed benchmark results and methodologies will be populated here as they become available.</p>"}]}